---
title: Hadoop环境搭建步骤
icon: computer
order: 10
category:
  - 大数据
tag:
  - hadoop
data: 2025-09-11
---

本节为 Hadoop 环境搭建步骤

<!-- more -->

## 一、前置准备：VirtualBox 与 Ubuntu 配置（基础环境）

伪分布式需 Ubuntu 虚拟机（推荐 20.04 LTS 版本，内存 ≥ 2GB、硬盘 ≥ 20GB），先完成基础配置。

### 1、安装 VirtualBox 与 Ubuntu

- 下载 VirtualBox 和 Ubuntu 20.04 ISO，按向导新建虚拟机（系统类型选“Linux → Ubuntu 64-bit”）。
- 安装 Ubuntu 时，建议设置 固定用户名和密码。

### 2、配置虚拟机网络（关键）

- 关闭虚拟机，进入“设置 → 网络”：

- 网卡 1：选“网络地址转换（NAT）”，确保虚拟机能连外网（用于下载软件）；

- 网卡 2：选“仅主机（Host-Only）适配器”，确保主机与虚拟机互通（后续 SSH 连接方便）。

- 启动 Ubuntu，执行 `ip addr` 查看网卡 2 的 IP（如 192.168.56.101 ），记下来备用。

### 3、安装基础工具

打开终端，执行命令安装 SSH（远程连接）和 Java（Hadoop 依赖）：

```bash
sudo apt update  # 更新软件源

sudo apt install -y openssh-server openjdk-8-jdk  # 安装 SSH 和 JDK8（Hadoop 推荐版本）
```

## 二、核心配置 1：SSH 免密登录（Hadoop 内部通信依赖）

伪分布式中 Hadoop 进程需通过 SSH 通信，免密登录可避免频繁输密码。

### 1、生成 SSH 密钥

终端执行（当前用户为 hadoop）：

```bash
ssh-keygen -t rsa  # 按3次回车，生成密钥（默认存于 ~/.ssh/ 目录）
```

- 连续按 3 次回车（默认存储路径～/.ssh/，无密码），生成私钥`id_rsa`和公钥`id_rsa.pub`。

### 2、授权本地免密登录

- 将公钥添加到授权列表，允许自身 SSH 登录：

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

- 修复文件权限（SSH 要求授权文件权限必须为 600，否则拒绝登录）：

```bash
chmod 600 ~/.ssh/authorized_keys  # 修复权限（否则 SSH 会拒绝）
```

### 3、测试免密登录

- 执行`ssh localhost`，首次连接会提示 “Are you sure you want to continue connecting (yes/no)?”，输入`yes`；
- 若无需输入密码即可登录（显示 “Welcome to Ubuntu...”），则成功；
- 输入`exit`退出 SSH 连接。

## 三、核心配置 2：Java 环境变量（Hadoop 需定位 JDK）

**目标**：让 Hadoop 通过环境变量找到 JDK 路径，避免启动报错。

### 1、查看 JDK 安装路径

- 执行：

```bash
update-alternatives --config java
```

- 输出类似：

```
0 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java 1081 auto mode
```

复制 “/usr/lib/jvm/java-8-openjdk-amd64”（去掉末尾的 “/jre/bin/java”）。

### 2、配置环境变量

- 编辑用户级配置文件（仅当前 hadoop 用户生效）：

```bash
nano ~/.bashrc  # 打开配置文件
```

- 按 Ctrl+End 跳至文件末尾，添加：

```bash
# Java环境变量
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64  # 替换为JDK实际路径
export JRE_HOME=$JAVA_HOME/jre
export PATH=$PATH:$JAVA_HOME/bin
```

- 按 Ctrl+O 保存，Ctrl+X 退出。

### 3、生效配置

- 执行`source ~/.bashrc`（使配置生效）；
- 执行`echo $JAVA_HOME`，若输出 “/usr/lib/jvm/java-8-openjdk-amd64”（与配置一致），则说明配置成功。

## 四、核心配置 3：Hadoop 下载与伪分布式配置

这是最关键步骤，需修改 4 个核心配置文件，确保 Hadoop 以“伪分布式”模式运行。

### 1、下载并解压 Hadoop

#### 下载 Hadoop 3.3.x

- 推荐 3.3.6 版本（稳定兼容 JDK8），终端执行：

```bash
wget https://archive.apache.org/dist/hadoop/core/hadoop-3.3.6/hadoop-3.3.6.tar.gz
```

（若 wget 下载慢，可在主机浏览器下载后，通过虚拟机 “共享文件夹” 或 SFTP 工具传到 Ubuntu 的～/Downloads 目录，再用 mv ~/Downloads/hadoop-3.3.6.tar.gz .移到当前目录）。

- 或者使用镜像地址下载（更推荐）：

```bash
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
```

#### 解压并配置路径

- 解压到 /opt 目录（统一管理）：

```bash
sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/  # -zxvf：解压.gz文件；-C：指定目标目录
```

- 重命名为 hadoop（简化路径）：

```bash
sudo mv /opt/hadoop-3.3.6 /opt/hadoop
```

- 赋予当前用户权限（否则无法修改配置文件）：

```bash
sudo chown -R hadoop:hadoop /opt/hadoop  # 递归修改所有者为hadoop用户和用户组
```

如果用户为其他，则需要修改`hadoop:hadoop`为当前用户

### 2、配置 Hadoop 环境变量

- 编辑～/.bashrc 添加 Hadoop 变量：

```bash
nano ~/.bashrc
```

- 末尾添加：

```bash
# Hadoop环境变量
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin  # 添加bin和sbin到PATH（可直接执行hadoop命令）
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop  # 配置文件目录
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"  # 加载本地库
```

- 保存退出后，执行`source ~/.bashrc`生效；
- 验证：执行`hadoop version`，若显示 “Hadoop 3.3.6” 及版本信息，则成功。

### 3、修改 Hadoop 核心配置文件（共 5 个）

所有配置文件均在 /opt/hadoop/etc/hadoop/ 目录下，用 nano 编辑。

#### hadoop-env.sh（指定 JDK 路径）

- 执行`nano /opt/hadoop/etc/hadoop/hadoop-env.sh`；
- 搜索`# export JAVA_HOME=`（按 Ctrl+W 输入 “JAVA_HOME” 查找），去掉注释并替换为实际 JDK 路径：

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```

- 保存退出。

#### core-site.xml（配置 HDFS 主节点）

- 执行`nano /opt/hadoop/etc/hadoop/core-site.xml`；
- 在`<configuration>`标签内添加（**注意：所有配置必须放在`<configuration>`内**）：

```xml
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>  <!-- HDFS主节点地址：localhost（本机）+ 端口9000 -->
</property>
<property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/hadoop/tmp</value>  <!-- Hadoop临时目录（需手动创建） -->
</property>
```

- 保存退出。

#### hdfs-site.xml（配置 HDFS 存储与副本数，伪分布式为 1）

- 先创建 NameNode 和 DataNode 的存储目录：

```bash
mkdir -p /home/hadoop/hdfs/name  # NameNode元数据目录
mkdir -p /home/hadoop/hdfs/data  # DataNode数据块目录
```

- 执行`nano /opt/hadoop/etc/hadoop/hdfs-site.xml`；
- 在`<configuration>`标签内添加：

```xml
<configuration>
    <!-- 1. NameNode 元数据存储目录（需手动创建该路径，如 /home/hadoop/hdfs/name） -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hadoop/hdfs/name</value>
        <!-- 说明："file:///" 表示本地文件系统，路径替换为你实际的存储目录（需提前用 mkdir -p 创建） -->
    </property>
    <!-- 2. DataNode 数据块存储目录（需手动创建该路径，如 /home/hadoop/hdfs/data） -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hadoop/hdfs/data</value>
        <!-- 说明：同 NameNode 路径，需提前创建，用于存储 HDFS 实际数据块 -->
    </property>
    <!-- 3. HDFS 文件副本数（单节点必须设为 1，多节点默认 3） -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
        <!-- 说明：单节点只有 1 个 DataNode，副本数设为 1 否则报错；多节点可设 2-3 保证数据冗余 -->
    </property>
</configuration>
```

- 保存退出。

#### mapred-site.xml（配置 MapReduce 运行框架）

- 若文件不存在（Hadoop 3.x 通常已存在，无需复制），复制模板：

```bash
cp /opt/hadoop/etc/hadoop/mapred-site.xml.template /opt/hadoop/etc/hadoop/mapred-site.xml
```

- 执行`nano /opt/hadoop/etc/hadoop/mapred-site.xml`；
- 在`<configuration>`标签内添加：

```xml
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>  <!-- 指定MapReduce运行在YARN上 -->
</property>
```

- 保存退出。

#### yarn-site.xml（配置 YARN 资源管理器）

- 执行 `nano /opt/hadoop/etc/hadoop/yarn-site.xml`；
- 在 `<configuration>` 标签内添加：

```xml
<property>
    <name>yarn.resourcemanager.address</name>
    <value>localhost:8032</value>  <!-- YARN主节点地址 -->
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>  <!-- 启用MapReduce洗牌服务（必须配置） -->
</property>
```

- 保存退出。

### 4、创建 Hadoop 临时目录

- 按 core-site.xml 配置，创建临时目录并授权：

```bash
mkdir -p /opt/hadoop/tmp  # 创建目录
chown -R hadoop:hadoop /opt/hadoop/tmp  # 确保hadoop用户有权限
```

## 五、格式化 HDFS 与启动集群

HDFS 首次使用前需格式化（==仅执行 1 次，重复执行会清空数据==），然后启动所有进程。

### 1、格式化 HDFS（仅首次执行，不可重复！）

- 格式化会清空 HDFS 数据，仅首次启动前执行：

```bash
hdfs namenode -format
```

- 成功标志：输出末尾显示 “INFO common.Storage: Storage directory ... has been successfully formatted”；
- 若失败：检查 hadoop-env.sh 中 JAVA_HOME 是否正确，或 HDFS 目录权限是否为 hadoop 用户所有。

### 2、启动 Hadoop 集群

#### 启动 HDFS（NameNode、DataNode、SecondaryNameNode）

- 执行：

```bash
start-dfs.sh
```

- 首次启动会提示 “Are you sure you want to continue connecting (yes/no)?”，输入`yes`（每个进程可能提示一次）。

#### 启动 YARN（ResourceManager、NodeManager）

- 执行：

```bash
start-yarn.sh
```

### 3、验证集群启动成功

- 执行`jps`（Java 进程查看命令），应显示以下进程：

  - NameNode
  - DataNode
  - SecondaryNameNode
  - ResourceManager
  - NodeManager

- 若缺少某进程：检查对应配置文件（如缺 DataNode，检查 hdfs-site.xml 路径是否正确；缺 ResourceManager，检查 yarn-site.xml）。

## 六、后续验证（可选）

- 访问 HDFS Web 界面：在主机浏览器输入`http://虚拟机IP:9870`（如 192.168.56.101:9870），可查看 HDFS 状态；
- 访问 YARN Web 界面：输入`http://虚拟机IP:8088`，可查看集群资源状态。

**注意事项**：

- 所有操作建议使用 “hadoop” 用户执行，避免权限问题；
- 关闭集群用`stop-dfs.sh`和`stop-yarn.sh`；
- 若需重启虚拟机，重启后需重新执行`start-dfs.sh`和`start-yarn.sh`启动集群。
